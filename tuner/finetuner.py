"""
Fine-tuning de mod√®les mBART pour la traduction biblique, verset par verset pour un batch.
"""

import os, glob
import time
import shutil
from datetime import datetime
from transformers import Trainer, TrainingArguments, EarlyStoppingCallback
import torch
from tuner.training_utils import load_model_for_training, preserve_chinese_punctuation, get_mbart_language_code
from tuner.translator import cleanup_memory
from tuner.metrics_calculator import calculate_training_metrics
from tuner.metadata_manager import save_training_metadata
from memory_manager.cleaner import cleanup_intermediate_files

def fine_tune_batch(model_path, source_text, target_text, batch_number,
                   source_language, target_language, corpus_texts, batch_reference):
    """
    Fine-tune un mod√®le mBART sur un batch complet de donn√©es bibliques.
    Tente d'abord l'optimisation automatique des hyperparam√®tres, puis fallback sur les param√®tres manuels.
    
    Args:
        model_path: Chemin vers le mod√®le de base
        source_text: Texte source pour l'ensemble du batch
        target_text: Texte cible pour l'ensemble du batch
        batch_number: Num√©ro du batch pour nommage
        source_language: Nom de langue source ("English", "Chinese", etc.)
        target_language: Nom de langue cible ("English", "Chinese", etc.)
        
    Returns:
        str: Chemin du mod√®le fine-tun√© (ou None si √©chec)
    """
    DISABLE_OPTUNA = False
    
    if not source_text or not target_text:
        print("‚ùå Textes source ou cible vides")
        return None
    
    # G√©n√©rer le chemin de sortie
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"outputs/models/batch_{batch_number}_{timestamp}"
    
    print(f"   Fine-tuning batch: {source_language} ‚Üí {target_language}")
    print(f"   Mod√®le: {model_path.split('/')[-1]} ‚Üí {output_path}")
    
    cleanup_memory()
    
    # Charger le mod√®le
    training_device = torch.device("cpu")
    model, tokenizer, device = load_model_for_training(model_path)
    if model is None:
        return None
        
    # Dataset pour Trainer
    class VerseDataset(torch.utils.data.Dataset):
        def __init__(self, source_verses, target_verses, tokenizer, source_language, target_language, device):
            self.examples = []
            
            src_code = get_mbart_language_code(source_language)
            tgt_code = get_mbart_language_code(target_language)
            
            for src_verse, tgt_verse in zip(source_verses, target_verses):
                # Configurer les langues
                tokenizer.src_lang = src_code
                tokenizer.tgt_lang = tgt_code
                
                # Post-traitement chinois si n√©cessaire
                if target_language == "Chinese":
                    tgt_verse = preserve_chinese_punctuation(tgt_verse)
                
                # Tokeniser chaque verset individuellement
                inputs = tokenizer(
                    src_verse,
                    max_length=512,
                    truncation=True,
                    padding='max_length',
                    return_tensors='pt'
                )
                
                labels = tokenizer(
                    tgt_verse,
                    max_length=512,
                    truncation=True,
                    padding='max_length',
                    return_tensors='pt'
                )
                
                self.examples.append({
                    'input_ids': inputs['input_ids'].squeeze(0).to(device),
                    'attention_mask': inputs['attention_mask'].squeeze(0).to(device),
                    'labels': labels['input_ids'].squeeze(0).to(device)
                })
        
        def __len__(self):
            return len(self.examples)
        
        def __getitem__(self, idx):
            return self.examples[idx]
    
    try:

        # R√©cup√©rer les versets individuels du batch
        batch_verses = []
        for verse_id, verse_data in corpus_texts.items():
            if (verse_id != "batch_data" and
                isinstance(verse_data, dict) and
                verse_data.get("batch_reference") == batch_reference):
                batch_verses.append({
                    'source': verse_data['source_text'],
                    'target': verse_data['target_text']
                })

        # Diviser les versets (85% train, 15% eval)
        split_idx = int(len(batch_verses) * 0.85)
        train_verses = batch_verses[:split_idx]
        eval_verses = batch_verses[split_idx:]

        
        # Pr√©parer les listes de versets
        train_source_verses = [v['source'] for v in train_verses]
        train_target_verses = [v['target'] for v in train_verses]
        eval_source_verses = [v['source'] for v in eval_verses]
        eval_target_verses = [v['target'] for v in eval_verses]

        # Cr√©er les datasets par verset
        train_dataset = VerseDataset(
            train_source_verses, train_target_verses,
            tokenizer, source_language, target_language, device
        )
        eval_dataset = VerseDataset(
            eval_source_verses, eval_target_verses,
            tokenizer, source_language, target_language, device
        )
        
        print(f"üìä Dataset cr√©√©: {len(train_dataset)} versets train, {len(eval_dataset)} versets eval")
        
        # Cr√©er le r√©pertoire de sortie
        os.makedirs(output_path, exist_ok=True)
        
        # Sauvegarder la liste des fichiers avant optimisation (pour supprimer les donn√©es d'essai d'optimisation)
        files_before = set(os.listdir(output_path)) if os.path.exists(output_path) else set()
        trainer = None
        final_model = None
        training_time = 0
        # Tenter l'optimisation automatique des hyperparam√®tres
        try:
            import optuna
            use_auto_hp = True
            print("üìä Tentative d'optimisation automatique des hyperparam√®tres avec Optuna...")
        except ImportError:
            use_auto_hp = False
            print("‚ö†Ô∏è  Optuna non disponible, utilisation des param√®tres manuels")
        
        if use_auto_hp and not DISABLE_OPTUNA:
            try:
                base_model, base_tokenizer, device = load_model_for_training(model_path)
                # Fonction de cr√©ation du mod√®le pour l'optimisation (r√©initialiser seulement les poids pour ne pas recharger le mod√®le √† chaque fois)
                def model_init(trial=None):
                    import copy
                    model_copy = copy.deepcopy(base_model)
                    return model_copy
                
                # D√©finir l'espace de recherche pour Optuna
                def optuna_hp_space(trial):
                    return {
                        "learning_rate": trial.suggest_float("learning_rate", 1e-6, 1e-4, log=True),
                        "num_train_epochs": trial.suggest_int("num_train_epochs", 3, 8),
                        "warmup_steps": trial.suggest_int("warmup_steps", 0, 20),
                        "weight_decay": trial.suggest_float("weight_decay", 0.0, 0.1),
                    }
                
                # Arguments de base pour l'optimisation
                training_args = TrainingArguments(
                    output_dir=output_path,
                    per_device_train_batch_size=1,
                    per_device_eval_batch_size=1,
                    eval_strategy="steps",
                    eval_steps=5,
                    save_steps=5,
                    logging_steps=2,
                    save_total_limit=2,
                    load_best_model_at_end=True,
                    metric_for_best_model="eval_loss",
                    greater_is_better=False,
                    remove_unused_columns=False,
                    save_safetensors=False,
                    dataloader_num_workers=0,
                    dataloader_pin_memory=False,
                )
                
                # Cr√©er le trainer pour l'optimisation
                trainer = Trainer(
                    model=None,
                    args=training_args,
                    train_dataset=train_dataset,
                    eval_dataset=eval_dataset,
                    processing_class=tokenizer,
                    model_init=model_init,
                    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
                )
                
                print(f"üîç Recherche des meilleurs hyperparam√®tres...")
                start_time = time.time()
                
                # Lancer la recherche automatique (nombre r√©duit de trials pour votre cas)
                best_trials = trainer.hyperparameter_search(
                    direction="minimize",
                    backend="optuna",
                    hp_space=optuna_hp_space,
                    n_trials=5,  # R√©duit pour √©viter des temps trop longs
                    compute_objective=lambda metrics: metrics.get("eval_loss", float('inf'))
                )
                
                training_time = time.time() - start_time
                print(f"‚úÖ Optimisation termin√©e en {training_time:.1f}s")
                print(f"üìà Meilleurs hyperparam√®tres: {best_trials.hyperparameters}")
                
                best_params = best_trials.hyperparameters
                model, tokenizer, device = load_model_for_training(model_path)
                
                # Arguments d'entra√Ænement avec les meilleurs param√®tres
                best_training_args = TrainingArguments(
                    output_dir=output_path,
                    per_device_train_batch_size=1,
                    per_device_eval_batch_size=1,
                    eval_strategy="steps",
                    eval_steps=5,
                    save_steps=5,
                    logging_steps=2,
                    save_total_limit=2,
                    load_best_model_at_end=True,
                    metric_for_best_model="eval_loss",
                    greater_is_better=False,
                    remove_unused_columns=False,
                    save_safetensors=False,
                    dataloader_num_workers=0,
                    dataloader_pin_memory=False,
                    **best_params  # Utiliser les param√®tres optimis√©s
                )
                
                # Cr√©er le trainer final avec les meilleurs param√®tres
                final_trainer = Trainer(
                    model=model,
                    args=best_training_args,
                    train_dataset=train_dataset,
                    eval_dataset=eval_dataset,
                    processing_class=tokenizer,
                    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
                )
                
                print(f"üéØ Entra√Ænement final avec les meilleurs param√®tres...")
                start_final_time = time.time()
                final_trainer.train()
                final_training_time = time.time() - start_final_time
                training_time = final_training_time  # Remplacer le temps d'optimisation
                
                final_model = final_trainer.model
                trainer = final_trainer  # Pour la suite du code qui utilise trainer.state
                
                # Nettoyage des optimizer.pt d'Optuna
                cleanup_patterns = [
                    f"{output_path}/**/optimizer.pt",     # Dans le dossier du batch
                    f"{output_path}/**/pytorch_model.bin", # Dans le dossier du batch
                    "outputs/models/**/optimizer.pt",      # Tous les anciens aussi
                    "outputs/models/**/pytorch_model.bin"  # Tous les anciens aussi
                ]
                for pattern in cleanup_patterns:
                    files_to_clean = glob.glob(pattern, recursive=True)
                    for file_path in files_to_clean:
                        try:
                            os.remove(file_path)
                            print(f"üßπ Checkpoint Optuna supprim√©: {file_path}")
                        except Exception as e:
                            print(f"‚ö†Ô∏è Erreur suppression {file_path}: {e}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur dans l'optimisation automatique: {e}")
                print("üîÑ Basculement vers les param√®tres manuels...")
                use_auto_hp = False
                trainer = None
        
        # Fallback : utiliser les param√®tres manuels actuels
        if not use_auto_hp or trainer is None:
            # Arguments d'entra√Ænement manuels (vos param√®tres actuels)
            training_args = TrainingArguments(
                output_dir=output_path,
                num_train_epochs=5,
                per_device_train_batch_size=1,
                per_device_eval_batch_size=1,
                learning_rate=3e-5,
                warmup_steps=5,
                eval_strategy="steps",
                eval_steps=5,
                save_steps=5,
                logging_steps=2,
                save_total_limit=2,
                load_best_model_at_end=True,
                metric_for_best_model="eval_loss",
                greater_is_better=False,
                remove_unused_columns=False,
                save_safetensors=False,
                weight_decay=0.01,
                dataloader_pin_memory=False,
            )
            
            # Cr√©er le trainer avec param√®tres manuels
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                processing_class=tokenizer,
                callbacks=[
                    EarlyStoppingCallback(early_stopping_patience=2)
                ]
            )
            
            print(f"üìö Training sur batch complet avec param√®tres manuels...")
            start_time = time.time()
            trainer.train()
            training_time = time.time() - start_time
            final_model = trainer.model
            
        if trainer is None:
            raise Exception("Impossible de cr√©er le trainer")
        if final_model is None:
            final_model = trainer.model
        
        # Calculer les m√©triques de training
        training_metrics = calculate_training_metrics(
            train_history=trainer.state.log_history,
            training_time=training_time,
            source_language=source_language,
            target_language=target_language
        )
        
        # Sauvegarder le mod√®le final
        try:
            # Essai de safetensors
            final_model.save_pretrained(
                output_path,
                safe_serialization=True,
                max_shard_size="5GB"  # √âvite les fichiers trop volumineux
            )
            print(f"‚úÖ Mod√®le sauv√© avec safetensors")
        except Exception as e:            # Fallback : sauvegarde PyTorch compl√®te
            print(f"‚ö†Ô∏è Erreur safetensors: {e}")
            print(f"üîÑ Basculement vers sauvegarde PyTorch compl√®te...")
            final_model.save_pretrained(
                output_path,
                safe_serialization=False,  # Force la sauvegarde compl√®te
                max_shard_size="5GB"
            )
            print(f"‚úÖ Mod√®le sauv√© avec PyTorch format")

        # Tokenizer (toujours sauver)
        tokenizer.save_pretrained(output_path)
        files_after_model_save = set(os.listdir(output_path)) # pour nettoyage
        legitimate_files = files_after_model_save - files_before
        
        # Sauvegarder les m√©tadonn√©es
        save_training_metadata(
            batch_number=batch_number,
            model_path=model_path,
            output_model=output_path,
            source_text=source_text,
            target_text=target_text,
            source_language=source_language,
            target_language=target_language,
            training_metrics=training_metrics
        )
        
        # Nettoyer les fichiers temporaires cr√©√©s pendant l'optimisation
        if use_auto_hp:
            all_files_final = set(os.listdir(output_path))
            temp_files = all_files_final - files_before - legitimate_files
            for temp_file in temp_files:
                temp_path = os.path.join(output_path, temp_file)
                try:
                    if os.path.isdir(temp_path):
                        shutil.rmtree(temp_path)
                    else:
                        os.remove(temp_path)
                    print(f"üßπ Supprim√© fichier temporaire: {temp_file}")
                except Exception as cleanup_error:
                    print(f"‚ö†Ô∏è  Impossible de supprimer {temp_file}: {cleanup_error}")
                                
        print(f"‚úÖ Fine-tuning termin√©: {output_path}")
        return output_path, training_metrics
        
    except Exception as e:
        print(f"‚ùå Erreur pendant le fine-tuning: {e}")
        return None
    
    finally:
        cleanup_memory()
