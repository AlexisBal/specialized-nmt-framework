"""
Translator
Traducteur utilisant mBART pour la traduction des textes bibliques.
"""

import os
import torch
import gc
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
from tuner.training_utils import preserve_chinese_punctuation, get_mbart_language_code, get_training_device

# Configuration pour r√©duire les warnings
os.environ["TOKENIZERS_PARALLELISM"] = "warning"
os.environ["TRANSFORMERS_VERBOSITY"] = "error"


def load_model_and_tokenizer(model_path, force_device=None):
    """
    Charge le mod√®le et le tokenizer mBART depuis un chemin donn√©.
    
    Args:
        model_path: Chemin vers le mod√®le (local ou HuggingFace)
        
    Returns:
        tuple: (model, tokenizer, device)
    """
    if force_device:
        device = force_device
    else:
        device = torch.device("cpu")
    
    try:
        # Charger le mod√®le et tokenizer
        model = MBartForConditionalGeneration.from_pretrained(model_path)
        tokenizer = MBart50TokenizerFast.from_pretrained(model_path)
        
        # D√©placer le mod√®le sur le device appropri√©
        model = model.to(device)
        model.eval()  # Mode √©valuation pour l'inf√©rence
        
        print(f"‚úÖ Mod√®le charg√©: {model_path.split('/')[-1]} sur {device}")
        return model, tokenizer, device
        
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du mod√®le {model_path}: {e}")
        return None, None, None


def cleanup_memory():
    """Nettoie la m√©moire GPU/CPU"""
    gc.collect()

def translate(model_path, source_text, source_language, target_language,
              tokenizer=None, model=None, device=None):
    """
    Traduit un texte donn√© en utilisant mBART.
    
    Args:
        model_path: Chemin vers le mod√®le mBART (utilis√© seulement si model=None)
        source_text: Texte √† traduire
        source_language: Nom de langue source ("English", "Chinese", etc.)
        target_language: Nom de langue cible ("English", "Chinese", etc.)
        tokenizer: Tokenizer optionnel (si None, sera charg√©)
        model: Mod√®le optionnel (si None, sera charg√© depuis model_path)
        device: Device optionnel (si None, sera d√©tect√©)
        
    Returns:
        str: Texte traduit avec ponctuation appropri√©e
    """
    
    if not source_text or not source_text.strip():
        print("‚ö†Ô∏è Texte source vide")
        return ""
    
    # Convertir les noms de langues en codes mBART
    src_code = get_mbart_language_code(source_language)
    tgt_code = get_mbart_language_code(target_language)
    
    # Charger si pas d√©j√† fourni
    load_model_needed = (model is None)
    
    if load_model_needed:
        model, tok, device = load_model_and_tokenizer(model_path)
        if model is None:
            return ""
        
        # Utiliser le tokenizer fourni ou celui charg√©
        if tokenizer is None:
            tokenizer = tok
    
    try:
        # Pr√©parer les inputs
        inputs = tokenizer(
            source_text,
            return_tensors="pt",
            max_length=1024,
            truncation=True,
            padding=True
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Configurer les langues
        tokenizer.src_lang = src_code
        tokenizer.tgt_lang = tgt_code
                
        # G√©n√©ration
        with torch.no_grad():
            generated_tokens = model.generate(
                **inputs,
                forced_bos_token_id=tokenizer.lang_code_to_id[tgt_code],
                max_new_tokens=512,
                min_length=10,
                num_beams=5,
                do_sample=False,
                repetition_penalty=1.1,
                no_repeat_ngram_size=3,
                early_stopping=True,          
                pad_token_id=tokenizer.pad_token_id
            )

        # D√©coder le r√©sultat
        translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
        
        # Post-traitement sp√©cifique au chinois
        if target_language == "Chinese":
            translation = preserve_chinese_punctuation(translation)
        
        print(f"üî§ Traduction: {len(source_text)} ‚Üí {len(translation)} chars")
        return translation
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la traduction: {e}")
        return ""
    
    finally:
        if load_model_needed:
            cleanup_memory()


def translate_batch_texts(model_path, source_texts, source_language, target_language):
    """
    Traduit une liste de textes pour un batch
    """
    
    if not source_texts:
        return []
    
    # Charger le mod√®le UNE SEULE FOIS
    model, tokenizer, device = load_model_and_tokenizer(model_path)
    if model is None or tokenizer is None or device is None:
        print(f"‚ùå √âchec chargement mod√®le: model={model}, tokenizer={tokenizer}, device={device}")
        return [""] * len(source_texts)
    
    translations = []
    
    try:
        cleanup_memory()
        
        for text in source_texts:
            if not text or not text.strip():
                translations.append("")
                continue
                
            # Passer le mod√®le d√©j√† charg√©
            translation = translate(
                model_path=model_path,  # Pas utilis√© car model fourni
                source_text=text,
                source_language=source_language,
                target_language=target_language,
                tokenizer=tokenizer,    # R√©utilise le tokenizer
                model=model,           # R√©utilise le mod√®le
                device=device          # R√©utilise le device
            )
            translations.append(translation)
        
        return translations
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la traduction par batch: {e}")
        return [""] * len(source_texts)
    
    finally:
        cleanup_memory()  # Nettoie √† la fin du batch
