"""
Extractor
Extrait les caractÃ¨res chinois pour un texte donnÃ©
Version avec Stanza + fallback vers mÃ©thode brute si Ã©chec
"""
import re

# Cache global pour Ã©viter de recharger le modÃ¨le Stanza Ã  chaque appel
_stanza_nlp = None
_stanza_available = None
_extracted_proper_nouns = set()

def extract_chinese_words(text):
    """
    Extraction des mots chinois avec Stanza, fallback vers mÃ©thode brute
    Garde la mÃªme signature que la version originale
    """
    global _stanza_nlp, _stanza_available
    
    # Tentative Stanza en prioritÃ©
    if _stanza_available is None:  # Premier appel
        try:
            import stanza
            # Initialiser le modÃ¨le Stanza une seule fois
            _stanza_nlp = stanza.Pipeline('zh-hant', processors='tokenize', verbose=False)
            _stanza_available = True
            print("âœ… Stanza initialisÃ© pour extraction chinoise")
        except Exception as e:
            print(f"âš ï¸ Stanza non disponible: {e}")
            print("ğŸ”„ Fallback vers mÃ©thode brute")
            _stanza_available = False
    
    # Utiliser Stanza si disponible
    if _stanza_available and _stanza_nlp is not None:
        try:
            return _extract_with_stanza(text)
        except Exception as e:
            print(f"âš ï¸ Erreur Stanza: {e}, fallback vers mÃ©thode brute")
            return _extract_chinese_words_brute(text)
    
    # Fallback vers mÃ©thode brute
    return _extract_chinese_words_brute(text)


def _extract_with_stanza(text):
    """Extraction avec Stanza (mÃ©thode optimale)"""
    global _stanza_nlp
    
    doc = _stanza_nlp(text)
    words = set()
    proper_nouns = set()  # Identifier les noms propres
    
    for sentence in doc.sentences:
        for token in sentence.tokens:
            word = token.text.strip()
            
            # Appliquer les mÃªmes filtres que la mÃ©thode brute pour cohÃ©rence
            if (len(word) >= 1 and
                not word.isdigit() and
                not re.search(r'[ï¼Œã€‚ï¼šï¼›ï¼ï¼Ÿã€Œã€ï¼ˆï¼‰ã€""''â€¦â€”Â·]', word) and
                not word.startswith(('æ˜¯', 'å°±', 'çš„', 'äº†', 'åˆ')) and
                not word.endswith(('æ˜¯', 'å°±', 'çš„', 'äº†', 'åˆ')) and
                not any(c in 'çš„äº†æ˜¯åœ¨å°±ä¹Ÿéƒ½' for c in word)):
                words.add(word)
                
            # VÃ©rifier si c'est un nom propre
            for sent_word in sentence.words:
                if sent_word.text == word and sent_word.upos == 'PROPN':
                    proper_nouns.add(word)
                    break
    
    # Stocker les noms propres dans une variable globale
    global _extracted_proper_nouns
    _extracted_proper_nouns = proper_nouns
    
    return words


def _extract_chinese_words_brute(text):
    """
    MÃ©thode brute originale (fallback)
    Extraction simple des mots chinois par n-grammes
    """
    chars = list(text)
    words = set()
    _extracted_proper_nouns = set()
    
    # Extraire tous les segments de 1 Ã  4 caractÃ¨res
    for i in range(len(chars)):
        for length in [2, 3, 4, 1]:
            if i + length <= len(chars):
                word = ''.join(chars[i:i+length]).strip()
                
                # Filtres simples et universels
                if (len(word) >= 1 and  # Garder mÃªme les caractÃ¨res uniques
                    not word.isdigit() and  # Pas de nombres
                    ' ' not in word and  # Pas d'espaces
                    not re.search(r'[ï¼Œã€‚ï¼šï¼›ï¼ï¼Ÿã€Œã€ï¼ˆï¼‰ã€""''â€¦â€”Â·]', word) and  # Ponctuation complÃ¨te
                    not word.startswith(('æ˜¯', 'å°±', 'çš„', 'äº†', 'åˆ')) and
                    not word.endswith(('æ˜¯', 'å°±', 'çš„', 'äº†', 'åˆ')) and
                    not any(c in 'çš„äº†æ˜¯åœ¨å°±ä¹Ÿéƒ½' for c in word)):  # filtre les caractÃ¨res grammaticaux les plus frÃ©quents
                    words.add(word)
    return words
